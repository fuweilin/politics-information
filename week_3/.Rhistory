VAP2008 <- VAP[14]
VAP2008
adVAP2008 <- VAP2008 - felons[c(14)] - non[c(14)]
adVAP
adVAP2008 <- VAP2008 - felons[14] - non[14]
adVAP
adVAP2008
adVAP2008 <- VAP[c(14)] - felons[c(14)] - non[c(14)]
adVAP2008
adtotal2008 <- total[c(14)] - osvoters[c(14)]
adturnout1 <- adtotal2008 / adVAP2008
adturnout1
library(swirl)
swirl
rm(list=ls())
swirl
swirl()
5 + 7
x <- 5 + 7
x
y <- x - 3
y
c(1.1, 9, 3.14)
z <- c(1.1, 9, 3.14)
?c
z
z, 555, z
c(z, 555, z)
z * 2 +100
my_sqrt <- z - 1
info()
my_sqrt <- sqrt(z-1)
my_sqrt
my_div <- z / my_sqrt
my_div
a <- c(1, 2, 3, 4)
a <- c(1, 2, 3, 4) + c(1, 100)
c(1, 2, 3, 4) + c(0, 100)
info()
c(1, 2, 3, 4) + c(0, 10)
c(1, 2, 3, 4) + c(0, 10, 100)
z * 2 +1000
my_div
getwd()
ls()
x <- 9
ls()
list.files()
?list.files
args(list.files())
args(list.files)
old.dir <-
old.dir <- list.files()
old.dir <0 getwd()
old.dir <- getwd()
info()
bye()
install.packages("stm")
library(readr)
getwd()
setwd("Users/Jaylene/Desktop/grade2/empirical")
setwd("Users/Jaylene/Desktop/grade2")
/
setwd("Users/Jaylene/Desktop/grade2")
setwd("Users/Jaylene/Desktop")
achen1 <- read_file("Achen.txt")
setwd("~/")
getwd()
setwd("C:/Users/Jaylene/Desktop/grade2/empirical")
getwd()
achen1 <- read_file("Achen.txt")
achen1 <- gsub("\r", "", achen1)
install.packages("wordcould")
install.packages("wordcloud2")
?wordcould2
??wordcloud2
?wordcloud2
install.packages("wordcloud2")
library(wordcloud2)
wordcloud2(achen1, max.words=100)
wordcloud2(achen1, 100)
?wordcloud2
achen2 <- read_file("Achen2.txt")
library(wordcloud2)
wordcloud2(achen2)
library(htmltools)
install.packages("tm")
library(tm)
library(wordcloud)
install.packages("wordcloud")
library(wordcloud)
wordcloud2(achen2, max.words=10)
wordcloud(achen2, max.words=10)
install.packages("texteffect")
rm(ls)
rm(list=ls())
turnout <- read.csv("/Users/Jaylene/Desktop/grade2/empirical/turnout.csv")
# Q1
summary(turnout)
head(turnout)
year <- turnout$year
summary(year)
range(year)
median(year)
mean(year)
# Q2
total <- turnout$total
VEP <- turnout$VEP
VAP <- turnout$VAP
overseas <- turnout$overseas
osvoters <- turnout$osvoters
VAPOS <- overseas + VAP
turnout1 <- total / VAPOS
turnout1
turnout2<- total / VEP
turnout2
# Q3
ANES <- turnout$ANES / 100
ANES
dif1 <- ANES - turnout1
dif1
mean(dif1)
range(dif1)
dif2 <- ANES - turnout2
dif2
mean(dif2)
range(dif2)
# Q4
Pdif <- dif2[c(1, 3, 5, 7, 9, 11, 13, 14)]
Pdif
summary(Pdif)
Mdif <- dif2[c(2, 4, 6, 8, 10, 12)]
Mdif
summay(Mdif)
summary(Mdif)
# Q5
difp1 <- dif2[c(1:7)]
list(difp1)
summary(difp1)
difp2 <- dif2[c(8:14)]
list(difp2)
summary(difp2)
difp2 <- dif2[c(8:14)]
list(difp2)
summary(difp2)
# Q6
felons <- turnout$felons
non <- turnout$noncit
adVAP2008 <- VAP[c(14)] - felons[c(14)] - non[c(14)]
adVAP2008
adtotal2008 <- total[c(14)] - osvoters[c(14)]
adturnout1 <- adtotal2008 / adVAP2008
adturnout1
addif <- ANES[C(14)] - adturnout1
addif <- turnout$ANES[C] /  100 - adturnout1
ANES
ANES[14]
addif <- ANES[14] - adturnout1
addif
addifANES <- ANES[14] - adturnout1
addifANES
addifVAP <- turnout1 - adturnout1
addifVAP
addifVAP <- adturnout1 - turnout1[14]
addifVAP
addifVEP <- adturnout1 - turnout2[14]
addifVEP
dif1[14]
dif2[14]
swirl()
dir.create(testdir)
tesdir <- dir.create(old.dir)
dir.create("testdir")
setwd("testdir")
file.create("mytest.R")
list.files(setwd)
list.files()
file.exists("test.R")
file.exists("MYtest.R")
file.exists("mytest.R")
file.info("mytest.R")
file.rename(mytest.R, mytest2.R)
file.rename("mytest.R, mytest2.R")
file.rename("mytest.R", "mytest2.R")
file.remove('mytest.R')
file.copy("mytest2.R", "mytest3.R")
file.path("mytest3.R")
'folder1'
file.path('folder1', 'folder2')
?dir.create
file.path("testdir2", dir.create("testdir3"))
dir.create(file.path('testdir2','testdir3'), recursive = TRUE)
setwd
setwd(old.dir)
### Crawler_Example with rvest    #####################################################################
# 參考：https://blog.gtwang.org/r/rvest-web-scraping-with-r/
rm(list = ls())
library(rvest)
# Set url
url <- "https://news.google.com/news/?ned=zh-tw_tw&gl=TW&hl=zh-tw"
# Get response
res <- read_html(url)
### Crawler_Example with rvest    #####################################################################
# ???ttps://blog.gtwang.org/r/rvest-web-scraping-with-r/
rm(list = ls())
library(rvest)
# Set url
url <- "https://www.ptt.cc/bbs/NBA/index5720.html"
# Get response
res <- read_html(url)
# Parse the content and extract the titles
raw.titles <- res %>% html_nodes("div.title")
# Extract link
nba.article.link <- raw.titles %>% html_node("a") %>% html_attr('href')
# Extract article
nba.article.title <- raw.titles %>% html_text()
# Create dataframe
nba.df <- data.frame(nba.article.title, nba.article.link)
# Set df's colnames
nba.df.col.names <- c("title", "link")
colnames(nba.df) <- nba.df.col.names
View(nba.df)
library(jsonlite)
url <- "https://www.dcard.tw/_api/posts?popular=true"
res <- fromJSON(url)
View(res)
library(httr)
library(rvest)
library(xml2)
library(gsubfn)
library(magrittr)
library(dplyr)
install.packages(gsubfn)
install.packages("gsubfn")
library(gsubfn)
#=========================================================================
# live market's url :
targeturl <- "https://www.buy123.com.tw"
res <- GET(targeturl) %>% content("parse")
xpath = '//*[@id="container"]/div[4]/section[2]'
text1 <- res %>% html_node(xpath=xpath) %>% html_children()
url <- unlist(lapply(as.character(text1), strapplyc, 'href=\"(.*)#?ref'))
url <- paste0('https://www.buy123.com.tw/',url)
#=========================================================================
# set function for lapply :
webpage_parser <- function(x, xpath){
res_tmp <- GET(x) %>% content("parse")
res_tmp %>% html_node(xpath=xpath) %>% html_text()
}
#-------------------------------------------------------------------------
starttime <- Sys.time()
xpath='//*[@id="deal_detail_info"]/div[1]/div/h1'
text1 <- unlist(lapply(url, function(x) webpage_parser(x, xpath=xpath)))
xpath='//*[@id="price"]'
text2 <- unlist(lapply(url, function(x) webpage_parser(x, xpath=xpath)))
text2 <- gsub('[\r\n\t]', '', text2)
xpath='//*[@id="deal_price_detail"]/div[7]'
text3 <- unlist(lapply(url, function(x) webpage_parser(x, xpath=xpath)))
webdata <- paste(text1,text2,text3,sep='__')
runtime <- Sys.time() - starttime
runtime
#=========================================================================
# save data
ws_df <- data.frame(date=starttime, item=webdata)
ws_df <- ws_df %>% separate(item, c('item','price','num_of_people'), sep="__")
View(ws_df)
save(ws_df, file='C:\\Users\\cby2\\Desktop\\ws.RData')
library(httr)
url <- "http://ecshweb.pchome.com.tw/search/v3.3/all/results?q=sony&page=1&sort=rnk/dc"
res = GET(url)
res_json = content(res)
do.call(rbind,res_json$prods)
View(data.frame(do.call(rbind,res_json$prods)))
library(rvest)
title=read_html("https://news.google.com/news/headlines?hl=zh-TW&ned=zh-tw_tw&gl=TW")
title=html_text(title)
title=html_nodes(title,".boxTitle .listA .list_title")
title=html_nodes(title, "list_title")
title=read_html("https://news.google.com/news/headlines?hl=zh-TW&ned=zh-tw_tw&gl=TW")
View(title)
title=read_html("https://news.google.com/news/headlines/section/topic/WORLD?ned=zh-tw_tw&hl=zh-TW&gl=TW")
View(title)
read_html("https://news.google.com/news/headlines/section/topic/WORLD?ned=zh-tw_tw&hl=zh-TW&gl=TW")
library(rvest)
read_html("https://news.google.com/news/headlines/section/topic/WORLD?ned=zh-tw_tw&hl=zh-TW&gl=TW")
doc <- read_html("https://news.google.com/news/headlines/section/topic/WORLD?ned=zh-tw_tw&hl=zh-TW&gl=TW")
doc %>% html_nodes(".kWyHVd .ME7ew")
doc %>% html_nodes(".kWyHVd .ME7ew") %>% html_attr("href")
doc %>% html_nodes(".kWyHVd .ME7ew") %>% html_text()
View(doc)
# mtcars
library(ggplot2)
ggplot(mtcars, aes(x=wt, y=mpg)) + meon_point
ggplot(mtcars, aes(x=wt, y=mpg)) +
?ggplot
ggplot2
?ggplot2
ggplot(mtcars, aes(x=wt, y=mpg)) + geom_point()
ggplot(mtcars, aes(x=wt, y=mpg)) + geom_line() + geom_point()
for(i in c(1:9)){
paste("http://sports.ltn.com.tw/baseball",/i,sep"")
title=read_html("http://sports.ltn.com.tw/baseball")
#輸入你要爬取網頁的網址，爬取此網頁的html資訊
title=html_nodes(title,".boxTitle .listA .list_title")
#函數首先代入剛剛的網頁資訊，逗號後就是很重要的輸入你前面使用Selector複製的程式碼，他會從此網頁中篩選出剛剛你想要爬的新聞標題。
title
title=html_text(title)   # 只篩選出文字
title
title=iconv(title,"UTF-8")  #若是文字出現亂碼，將格式改成可以辨識的形式
url=read_html("http://sports.ltn.com.tw/baseball") #爬取同一個網頁
url=html_nodes(url,".boxTitle .listA a")
#a代表超連結
#這裡要注意，使用Selector選區塊時不要按在標題上，要點在旁邊，框框會比點在標題上面大喔，
#所以所選出來的程式碼和剛剛的不一樣。
url
url=html_attr(url,"href")  #選擇html頁面中的連結網址的部分(若使用剛剛爬標題的Selector碼會跑不出來，出現NA)
}
for(i in c(1:9)){
paste("http://sports.ltn.com.tw/baseball",/i,sep"")
title=read_html("http://sports.ltn.com.tw/baseball")
#輸入你要爬取網頁的網址，爬取此網頁的html資訊
title=html_nodes(title,".boxTitle .listA .list_title")
#函數首先代入剛剛的網頁資訊，逗號後就是很重要的輸入你前面使用Selector複製的程式碼，他會從此網頁中篩選出剛剛你想要爬的新聞標題。
title
title=html_text(title)   # 只篩選出文字
title
title=iconv(title,"UTF-8")  #若是文字出現亂碼，將格式改成可以辨識的形式
}
for(i in c(1:9)){
paste("http://sports.ltn.com.tw/baseball",/i,sep"")
title=read_html("http://sports.ltn.com.tw/baseball")
#輸入你要爬取網頁的網址，爬取此網頁的html資訊
title=html_nodes(title,".boxTitle .listA .list_title")
#函數首先代入剛剛的網頁資訊，逗號後就是很重要的輸入你前面使用Selector複製的程式碼，他會從此網頁中篩選出剛剛你想要爬的新聞標題。
title=html_text(title)   # 只篩選出文字
title=iconv(title,"UTF-8")  #若是文字出現亂碼，將格式改成可以辨識的形式
}
for(i in c(1:9)){
paste("http://sports.ltn.com.tw/baseball,/i,sep")
title=read_html("http://sports.ltn.com.tw/baseball")
#輸入你要爬取網頁的網址，爬取此網頁的html資訊
title=html_nodes(title,".boxTitle .listA .list_title")
#函數首先代入剛剛的網頁資訊，逗號後就是很重要的輸入你前面使用Selector複製的程式碼，他會從此網頁中篩選出剛剛你想要爬的新聞標題。
title=html_text(title)   # 只篩選出文字
title=iconv(title,"UTF-8")  #若是文字出現亂碼，將格式改成可以辨識的形式
}
for(i in c(1:9)){
paste("http://sports.ltn.com.tw/baseball,/i,sep")
title=read_html("http://sports.ltn.com.tw/baseball")
#輸入你要爬取網頁的網址，爬取此網頁的html資訊
title=html_nodes(title,".boxTitle .listA .list_title")
#函數首先代入剛剛的網頁資訊，逗號後就是很重要的輸入你前面使用Selector複製的程式碼，他會從此網頁中篩選出剛剛你想要爬的新聞標題。
title=html_text(title)   # 只篩選出文字
title=iconv(title,"UTF-8")  #若是文字出現亂碼，將格式改成可以辨識的形式
}
library(rvest)
for(i in c(1:9)){
paste("http://sports.ltn.com.tw/baseball,/i,sep")
title=read_html("http://sports.ltn.com.tw/baseball")
#輸入你要爬取網頁的網址，爬取此網頁的html資訊
title=html_nodes(title,".boxTitle .listA .list_title")
#函數首先代入剛剛的網頁資訊，逗號後就是很重要的輸入你前面使用Selector複製的程式碼，他會從此網頁中篩選出剛剛你想要爬的新聞標題。
title=html_text(title)   # 只篩選出文字
title=iconv(title,"UTF-8")  #若是文字出現亂碼，將格式改成可以辨識的形式
}
title
title
url=read_html("http://sports.ltn.com.tw/baseball") #爬取同一個網頁
url=html_nodes(url,".boxTitle .listA a")
#a代表超連結
#這裡要注意，使用Selector選區塊時不要按在標題上，要點在旁邊，框框會比點在標題上面大喔，
#所以所選出來的程式碼和剛剛的不一樣。
url
library(rvest)
for(i in c(1:9)){
paste("http://sports.ltn.com.tw/baseball,/i,sep")
title=read_html("http://sports.ltn.com.tw/baseball")
#輸入你要爬取網頁的網址，爬取此網頁的html資訊
title=html_nodes(title,".boxTitle .listA .list_title")
#函數首先代入剛剛的網頁資訊，逗號後就是很重要的輸入你前面使用Selector複製的程式碼，他會從此網頁中篩選出剛剛你想要爬的新聞標題。
title=html_text(title)   # 只篩選出文字
title=iconv(title,"UTF-8")  #若是文字出現亂碼，將格式改成可以辨識的形式
}
title
for(i in c(1:9)){
paste("http://sports.ltn.com.tw/baseball/", i, sep="")
title=read_html("http://sports.ltn.com.tw/baseball")
#輸入你要爬取網頁的網址，爬取此網頁的html資訊
title=html_nodes(title,".boxTitle .listA .list_title")
#函數首先代入剛剛的網頁資訊，逗號後就是很重要的輸入你前面使用Selector複製的程式碼，他會從此網頁中篩選出剛剛你想要爬的新聞標題。
title=html_text(title)   # 只篩選出文字
title=iconv(title,"UTF-8")  #若是文字出現亂碼，將格式改成可以辨識的形式
}
print(paste("http://sports.ltn.com.tw/baseball/", i, sep=""))
for(i in c(1:9)){
print(paste("http://sports.ltn.com.tw/baseball/", i, sep=""))
title=read_html("http://sports.ltn.com.tw/baseball")
#輸入你要爬取網頁的網址，爬取此網頁的html資訊
title=html_nodes(title,".boxTitle .listA .list_title")
#函數首先代入剛剛的網頁資訊，逗號後就是很重要的輸入你前面使用Selector複製的程式碼，他會從此網頁中篩選出剛剛你想要爬的新聞標題。
title=html_text(title)   # 只篩選出文字
title=iconv(title,"UTF-8")  #若是文字出現亂碼，將格式改成可以辨識的形式
}
title
library(rvest)
for(i in c(1:9)){
print(paste("http://sports.ltn.com.tw/baseball/", i, sep=""))
title=html_nodes(title,".boxTitle .listA .list_title")
#函數首先代入剛剛的網頁資訊，逗號後就是很重要的輸入你前面使用Selector複製的程式碼，他會從此網頁中篩選出剛剛你想要爬的新聞標題。
title=html_text(title)   # 只篩選出文字
title=iconv(title,"UTF-8")  #若是文字出現亂碼，將格式改成可以辨識的形式
}
rm(list=ls())
library(rvest)
for(i in c(1:9)){
print(paste("http://sports.ltn.com.tw/baseball/", i, sep=""))
title=html_nodes(title,".boxTitle .listA .list_title")
#函數首先代入剛剛的網頁資訊，逗號後就是很重要的輸入你前面使用Selector複製的程式碼，他會從此網頁中篩選出剛剛你想要爬的新聞標題。
title=html_text(title)   # 只篩選出文字
title=iconv(title,"UTF-8")  #若是文字出現亂碼，將格式改成可以辨識的形式
}
library(rvest)
for(i in c(1:9)){
url = paste("http://sports.ltn.com.tw/baseball/", i, sep="")
print(url)
# title=html_nodes(title,".boxTitle .listA .list_title")
# #函數首先代入剛剛的網頁資訊，逗號後就是很重要的輸入你前面使用Selector複製的程式碼，他會從此網頁中篩選出剛剛你想要爬的新聞標題。
# title=html_text(title)   # 只篩選出文字
# title=iconv(title,"UTF-8")  #若是文字出現亂碼，將格式改成可以辨識的形式
}
library(rvest)
for(i in c(1:9)){
url = paste("http://sports.ltn.com.tw/baseball/", i, sep="")
# print(url)
title=read_html(url)
title=html_nodes(title,".boxTitle .listA .list_title")
#函數首先代入剛剛的網頁資訊，逗號後就是很重要的輸入你前面使用Selector複製的程式碼，他會從此網頁中篩選出剛剛你想要爬的新聞標題。
title=html_text(title)   # 只篩選出文字
title=iconv(title,"UTF-8")  #若是文字出現亂碼，將格式改成可以辨識的形式
}
library(rvest)
for(i in c(1:9)){
url = paste("http://sports.ltn.com.tw/baseball/", i, sep="")
# print(url)
title=read_html(url)
title=html_nodes(title,".boxTitle .listA .list_title")
#函數首先代入剛剛的網頁資訊，逗號後就是很重要的輸入你前面使用Selector複製的程式碼，他會從此網頁中篩選出剛剛你想要爬的新聞標題。
title=html_text(title)   # 只篩選出文字
title=iconv(title,"UTF-8")  #若是文字出現亂碼，將格式改成可以辨識的形式
title
}
a = 5
a
library(rvest)
for(i in c(1:9)){
url = paste("http://sports.ltn.com.tw/baseball/", i, sep="")
# print(url)
title=read_html(url)
title=html_nodes(title,".boxTitle .listA .list_title")
#函數首先代入剛剛的網頁資訊，逗號後就是很重要的輸入你前面使用Selector複製的程式碼，他會從此網頁中篩選出剛剛你想要爬的新聞標題。
title=html_text(title)   # 只篩選出文字
title=iconv(title,"UTF-8")  #若是文字出現亂碼，將格式改成可以辨識的形式
print(title)
}
library(rvest)
for(i in c(1:9)){
url = paste("http://sports.ltn.com.tw/baseball/", i, sep="")
print(url)
title=read_html(url)
title=html_nodes(title,".boxTitle .listA .list_title")
#函數首先代入剛剛的網頁資訊，逗號後就是很重要的輸入你前面使用Selector複製的程式碼，他會從此網頁中篩選出剛剛你想要爬的新聞標題。
title=html_text(title)   # 只篩選出文字
title=iconv(title,"UTF-8")  #若是文字出現亂碼，將格式改成可以辨識的形式
print(title)
}
title
length(title)
pressure
mtcars
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
diamonds
ggplot(pressure, aex(x+temperature, y=pressure)) + geom_point()
ggplot(pressure, aes(x+temperature, y=pressure)) + geom_point()
ggplot(pressure, aes(x=temperature, y=pressure)) + geom_point()
ggplot(pressure, aes(x=temperature, y=pressure)) + geom_point() + geom_line()
SOD
iris
ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width)) + geom_line()
ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width)) + geom_line() + geom_point()
ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width)) + geom_histogram()
ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width)) + geom_boxplot()
ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width)) + geom_point() + geom_line()
diamonds
ggplot(diamonds, aes(x=cut)) + geom_bar()
ggplot(diamonds, aes(x=color)) + geom_bar()
ggplot(diamonds, aes(x=depth, y=price)) + geom_point()
ggplot(diamonds, aes(x=depth, y=price)) + geom_boxplot()
ggplot(diamonds, aes(x=clarity, y=price)) + geom_point()
ggplot(diamonds, aes(x=clarity, y=price)) + geom_line()
ggplot(diamonds, aes(x=cut, y=depth)) + geom_point()
ggplot(diamonds, aes(x=carat, y=depth)) + geom_point()
ggplot(diamonds, aes(x=cut, y=depth)) + geom_boxplot()
airquality
hist(airquality$Month,main="Histogram of Month", xlab="Month",  ylab="Frequency")  )
hist(airquality$Month, main="Histogram of Month", xlab="Month",  ylab="Frequency")  )
hist(airquality$Month, main="Histogram of Month", xlab="Month",  ylab="Frequency")
boxplot(formula = Ozone ~ Month, data = airquality, xlab = "Month", ylab = "Ozone (ppb)", col ="gray")
setwd()
getwd()
setwd()
setwd("C:/Users/Jaylene/Desktop/grade2/PI/politics-information/week_3")
